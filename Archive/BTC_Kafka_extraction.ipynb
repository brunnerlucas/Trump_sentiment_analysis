{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f92092b-b297-495b-ae06-3be392b9f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803a7079-f5c7-4034-881f-97ced547fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.bool = np.bool_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309700f8-0177-45e2-974d-00d64fdf32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#current notebook name\n",
    "notebook_name = __session__.replace('.ipynb','')[__session__.rfind('/')+1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67973726-1d15-40e7-85fc-9192f9ea9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS base paths\n",
    "hdfs_lakehouse_base_path = 'hdfs://localhost:9000/lakehouse/'\n",
    "hdfs_warehouse_base_path = 'hdfs://localhost:9000/warehouse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3791a4-67e3-4790-b421-2f46e2109d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dependencies = [\"org.apache.spark:spark-avro_2.12:3.5.0\",\n",
    "                \"io.delta:delta-iceberg_2.12:3.0.0\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\"]\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']= f\"--packages {','.join(dependencies)} pyspark-shell\"\n",
    "os.environ['PYARROW_IGNORE_TIMEZONE'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1239790c-4f9a-47be-a957-a49847e71b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/17 20:19:31 WARN Utils: Your hostname, osbdet resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s1)\n",
      "25/03/17 20:19:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/osbdet/.ivy2/cache\n",
      "The jars for the packages stored in: /home/osbdet/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "io.delta#delta-iceberg_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5c1e93d2-85ff-4ce5-a4b7-232da663b64b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.5.0 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound io.delta#delta-iceberg_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.1.1 in central\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.19.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.10.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 298ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.10.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-iceberg_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.19.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.1.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   0   |   0   |   0   ||   21  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5c1e93d2-85ff-4ce5-a4b7-232da663b64b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 21 already retrieved (0kB/4ms)\n",
      "25/03/17 20:19:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting Spark log level to \"ERROR\".\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize Spark Session with Kafka Support\n",
    "spark = (SparkSession.builder\n",
    "    .appName(notebook_name)\n",
    "    .config(\"spark.log.level\", \"ERROR\")\n",
    "    .config(\"spark.sql.warehouse.dir\", hdfs_warehouse_base_path)\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\")  # Enable schema inference for streaming data\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"  # Kafka integration\n",
    "            \"io.delta:delta-core_2.12:2.4.0\")  # Delta Lake support\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3119f9-7bb9-4895-8595-36bda20b9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Define schema for incoming Twitter data\n",
    "twitter_schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"favorites\", DoubleType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"isRetweet\", BooleanType(), True),\n",
    "    StructField(\"retweets\", DoubleType(), True),\n",
    "    StructField(\"text\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "twitter_stream = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"btc_price\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "# Extract JSON from Kafka's `value` column\n",
    "twitter_df = twitter_stream.withColumn(\n",
    "    \"parsed_value\", from_json(col(\"value\").cast(\"string\"), twitter_schema)\n",
    ").select(\"parsed_value.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aeb308f-4a11-4651-955c-93b18e9ae80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import requests\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, FloatType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "# Define HDFS path where the model is stored\n",
    "model = RandomForestRegressionModel.load(f\"{hdfs_lakehouse_base_path}/gold/trump_btc/rf_model\")\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "\n",
    "# Initialize Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "THRESHOLD_PERCENT = 5  # If predicted price is 5% above the current price, we BUY\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "\n",
    "    if not batch_df.isEmpty():\n",
    "        print(f\"🚀 Processing Batch {batch_id}\")\n",
    "\n",
    "        # Convert Spark DataFrame to Pandas for sentiment analysis\n",
    "        pdf = batch_df.toPandas()\n",
    "\n",
    "        # Apply Sentiment Analysis using VADER\n",
    "        pdf[\"sentiment_score\"] = pdf[\"text\"].apply(\n",
    "            lambda text: analyzer.polarity_scores(text)[\"compound\"] if isinstance(text, str) else 0.0\n",
    "        )\n",
    "\n",
    "        # Convert back to Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(pdf)\n",
    "\n",
    "        # Extract the **average** sentiment score for all tweets in this batch\n",
    "        avg_sentiment = spark_df.selectExpr(\"avg(sentiment_score) as avg_sentiment\").collect()[0][\"avg_sentiment\"]\n",
    "\n",
    "        # Fetch BTC & Fear/Greed Data and merge it with avg_sentiment\n",
    "        model_data, current_btc_price = fetch_btc_fng_for_model(spark, avg_sentiment)\n",
    "\n",
    "        # Convert feature columns into a vector column for Spark ML (Updated Columns)\n",
    "        feature_columns = [\"sentiment_score\", \"is_president\", \"Value\", \"BTC_Volume\", \"btc_current_price\"]\n",
    "        assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "        model_data = assembler.transform(model_data)\n",
    "\n",
    "        # Apply the trained Spark ML model for prediction\n",
    "        predictions = model.transform(model_data)\n",
    "\n",
    "        # Select final output columns\n",
    "        final_df = predictions.select(\n",
    "            \"sentiment_score\", \"is_president\", \"Value\", \"BTC_Volume\", \"btc_current_price\", col(\"prediction\").alias(\"predicted_btc_price\")\n",
    "        )\n",
    "\n",
    "        # Convert final DataFrame to Pandas for decision-making\n",
    "        final_pdf = final_df.toPandas()\n",
    "\n",
    "        # Extract predicted price\n",
    "        predicted_price = final_pdf[\"predicted_btc_price\"].iloc[0]\n",
    "\n",
    "        # Calculate percentage difference\n",
    "        percent_change = ((predicted_price - current_btc_price) / current_btc_price) * 100\n",
    "\n",
    "        # Trading Decision\n",
    "        if percent_change > THRESHOLD_PERCENT:\n",
    "            decision = \"🚀 BUY\"\n",
    "        else:\n",
    "            decision = \"❌ DO NOTHING\"\n",
    "\n",
    "        # Print trading decision\n",
    "        print(f\"Current BTC Price: ${current_btc_price:.2f}\")\n",
    "        print(f\"Predicted BTC Price: ${predicted_price:.2f}\")\n",
    "        print(f\"Price Change: {percent_change:.2f}%\")\n",
    "        print(f\"Trading Decision: {decision}\")\n",
    "\n",
    "        # Show predictions\n",
    "        final_df.show(truncate=False)\n",
    "\n",
    "def fetch_btc_fng_for_model(spark, sentiment_score):\n",
    "    \"\"\"\n",
    "    Fetches the latest BTC price, volume, and Fear & Greed Index, \n",
    "    and returns a Spark DataFrame with the required columns.\n",
    "\n",
    "    Parameters:\n",
    "        spark (SparkSession): Active Spark session.\n",
    "        sentiment_score (float): Average sentiment score from tweets.\n",
    "\n",
    "    Returns:\n",
    "        Spark DataFrame with selected columns for the ML model.\n",
    "        Current BTC price (float) for decision-making.\n",
    "    \"\"\"\n",
    "\n",
    "    # 🟢 Step 1: Get BTC Price and Volume\n",
    "    btc = yf.Ticker(\"BTC-USD\")\n",
    "    latest_data = btc.history(period=\"1d\")\n",
    "\n",
    "    if not latest_data.empty:\n",
    "        btc_current_price = float(np.random.uniform(2000, 10000)) #float(latest_data['Close'].iloc[-1])  # Current BTC price\n",
    "        latest_volume = float(latest_data['Volume'].iloc[-1])  # Current BTC volume\n",
    "        latest_date = latest_data.index[-1].strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        btc_current_price = None\n",
    "        latest_volume = None\n",
    "        latest_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    # 🟢 Step 2: Get Fear & Greed Index\n",
    "    response = requests.get(\"https://api.alternative.me/fng/?limit=1\")\n",
    "    data = response.json()\n",
    "\n",
    "    index_value = int(data['data'][0]['value'])  # Fear & Greed Index\n",
    "    classification = data['data'][0]['value_classification']\n",
    "\n",
    "    # 🟢 Step 3: Define Schema for Model DataFrame (Updated Columns)\n",
    "    schema = StructType([\n",
    "        StructField(\"sentiment_score\", FloatType(), True),\n",
    "        StructField(\"is_president\", IntegerType(), False),\n",
    "        StructField(\"Value\", IntegerType(), False),  # Renaming index_value as \"Value\"\n",
    "        StructField(\"BTC_Volume\", FloatType(), True),\n",
    "        StructField(\"btc_current_price\", FloatType(), True),\n",
    "    ])\n",
    "\n",
    "    # 🟢 Step 4: Create a Single Spark DataFrame\n",
    "    model_df = spark.createDataFrame([(sentiment_score, 1, index_value, latest_volume, btc_current_price)], schema=schema)\n",
    "\n",
    "    return model_df, btc_current_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c48338f-13ec-4816-975d-37d65f65ba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 1\n",
      "Current BTC Price: $2797.57\n",
      "Predicted BTC Price: $10808.41\n",
      "Price Change: 286.35%\n",
      "Trading Decision: 🚀 BUY\n",
      "+---------------+------------+-----+-------------+-----------------+-------------------+\n",
      "|sentiment_score|is_president|Value|BTC_Volume   |btc_current_price|predicted_btc_price|\n",
      "+---------------+------------+-----+-------------+-----------------+-------------------+\n",
      "|-0.9413        |1           |32   |2.66835702E10|2797.567         |10808.411351983068 |\n",
      "+---------------+------------+-----+-------------+-----------------+-------------------+\n",
      "\n",
      "🚀 Processing Batch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current BTC Price: $6064.85\n",
      "Predicted BTC Price: $9677.52\n",
      "Price Change: 59.57%\n",
      "Trading Decision: 🚀 BUY\n",
      "+---------------+------------+-----+-------------+-----------------+-------------------+\n",
      "|sentiment_score|is_president|Value|BTC_Volume   |btc_current_price|predicted_btc_price|\n",
      "+---------------+------------+-----+-------------+-----------------+-------------------+\n",
      "|0.3565         |1           |32   |2.66835702E10|6064.847         |9677.517685914816  |\n",
      "+---------------+------------+-----+-------------+-----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m twitter_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(process_batch) \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyter_venv/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.jupyter_venv/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.jupyter_venv/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query = twitter_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba579d-84c5-467b-9126-41fe7b06933b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439455e-4ead-4d1d-9241-51f348032054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
