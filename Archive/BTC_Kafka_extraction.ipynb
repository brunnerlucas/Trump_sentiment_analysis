{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f92092b-b297-495b-ae06-3be392b9f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803a7079-f5c7-4034-881f-97ced547fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.bool = np.bool_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309700f8-0177-45e2-974d-00d64fdf32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#current notebook name\n",
    "notebook_name = __session__.replace('.ipynb','')[__session__.rfind('/')+1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67973726-1d15-40e7-85fc-9192f9ea9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS base paths\n",
    "hdfs_lakehouse_base_path = 'hdfs://localhost:9000/lakehouse/'\n",
    "hdfs_warehouse_base_path = 'hdfs://localhost:9000/warehouse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3791a4-67e3-4790-b421-2f46e2109d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dependencies = [\"org.apache.spark:spark-avro_2.12:3.5.0\",\n",
    "                \"io.delta:delta-iceberg_2.12:3.0.0\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\"]\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']= f\"--packages {','.join(dependencies)} pyspark-shell\"\n",
    "os.environ['PYARROW_IGNORE_TIMEZONE'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1239790c-4f9a-47be-a957-a49847e71b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 14:29:37 WARN Utils: Your hostname, osbdet resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s1)\n",
      "25/03/16 14:29:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/osbdet/.ivy2/cache\n",
      "The jars for the packages stored in: /home/osbdet/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "io.delta#delta-iceberg_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7c747dc9-5c2f-488a-a53a-a9cc32fe5e77;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-avro_2.12;3.5.0 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound io.delta#delta-iceberg_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.1.1 in central\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.19.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.10.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 357ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.10.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-iceberg_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.19.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.1.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   0   |   0   |   0   ||   21  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7c747dc9-5c2f-488a-a53a-a9cc32fe5e77\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 21 already retrieved (0kB/8ms)\n",
      "25/03/16 14:29:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting Spark log level to \"ERROR\".\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize Spark Session with Kafka Support\n",
    "spark = (SparkSession.builder\n",
    "    .appName(notebook_name)\n",
    "    .config(\"spark.log.level\", \"ERROR\")\n",
    "    .config(\"spark.sql.warehouse.dir\", hdfs_warehouse_base_path)\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\")  # Enable schema inference for streaming data\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"  # Kafka integration\n",
    "            \"io.delta:delta-core_2.12:2.4.0\")  # Delta Lake support\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b47890-4993-4916-a3ad-eeb4f9bccfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "btc_price = spark.read.format(\"delta\").load(f\"{hdfs_lakehouse_base_path}/silver/trump_btc/BTC/\").cache()\n",
    "btc_stream_df = spark.readStream.format(\"delta\").load(f\"{hdfs_lakehouse_base_path}/silver/trump_btc/BTC/\")\n",
    "\n",
    "fg_df = spark.read.format(\"delta\").load(f\"{hdfs_lakehouse_base_path}/silver/trump_btc/fear_greed_index\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf39745-8453-4c07-94f9-1c8d1cbd799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      " |-- date_minute: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc_stream_df = btc_stream_df.withColumn(\"date_minute\", col(\"date\").cast(\"string\"))\n",
    "\n",
    "# Print schema to confirm\n",
    "btc_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b3119f9-7bb9-4895-8595-36bda20b9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Define schema for incoming Twitter data\n",
    "twitter_schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"favorites\", DoubleType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"isRetweet\", BooleanType(), True),\n",
    "    StructField(\"retweets\", DoubleType(), True),\n",
    "    StructField(\"text\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "twitter_stream = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"btc_price\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5fe424-c2ce-40d6-8655-faf9394922ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Extract JSON from Kafka's `value` column\n",
    "twitter_df = twitter_stream.withColumn(\n",
    "    \"parsed_value\", from_json(col(\"value\").cast(\"string\"), twitter_schema)\n",
    ").select(\"parsed_value.*\")\n",
    "\n",
    "# Initialize Vader Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    if text:  \n",
    "        return analyzer.polarity_scores(text)[\"compound\"]\n",
    "    return 0.0  \n",
    "\n",
    "\n",
    "# Define a function to process each micro-batch\n",
    "#def process_batch(batch_df, batch_id):\n",
    "#    if not batch_df.isEmpty():\n",
    "#        # Convert to Pandas DataFrame\n",
    "#        pdf = batch_df.toPandas()\n",
    "#\n",
    "#        # Apply sentiment analysis using Pandas\n",
    "#        pdf[\"sentiment_score\"] = pdf[\"text\"].apply(lambda text: analyzer.polarity_scores(text)[\"compound\"] if text else 0.0)\n",
    "\n",
    "#        # Convert back to Spark DataFrame\n",
    "#        spark_df = spark.createDataFrame(pdf)\n",
    "#\n",
    "#        # Write to Console (for debugging)\n",
    "#        spark_df.show(truncate=False)\n",
    "\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    if not batch_df.isEmpty():\n",
    "        # Extract unique 'date_minute' values from the batch\n",
    "        unique_dates = batch_df.select(\"date_minute\").distinct()\n",
    "\n",
    "        # Filter BTC data to include only relevant timestamps\n",
    "        filtered_btc_df = btc_stream_df.join(unique_dates, on=\"date_minute\", how=\"inner\")\n",
    "\n",
    "        # Perform join between batch tweets and BTC prices\n",
    "        enriched_df = batch_df.join(filtered_btc_df, on=\"date_minute\", how=\"inner\")\n",
    "\n",
    "        # Convert to Pandas DataFrame for sentiment analysis\n",
    "        pdf = enriched_df.toPandas()\n",
    "\n",
    "        # Apply sentiment analysis using Pandas\n",
    "        pdf[\"sentiment_score\"] = pdf[\"text\"].apply(lambda text: analyzer.polarity_scores(text)[\"compound\"] if text else 0.0)\n",
    "\n",
    "        # Convert back to Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(pdf)\n",
    "\n",
    "        # Select required columns and rename 'Close' as 'btc_price'\n",
    "        final_df = spark_df.select(\"date_minute\", \"favorites\", \"retweets\", \"text\", \"sentiment_score\", col(\"Close\").alias(\"btc_price\"))\n",
    "\n",
    "        # Write to console (or save to HDFS/Kafka if needed)\n",
    "        final_df.show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c18e2-d375-4d0a-ac50-97bbc7e66537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59893040-39ea-41f1-a448-a8f1a056c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Extract JSON from Kafka's `value` column\n",
    "twitter_df = twitter_stream.withColumn(\n",
    "    \"parsed_value\", from_json(col(\"value\").cast(\"string\"), twitter_schema)\n",
    ").select(\"parsed_value.*\")\n",
    "\n",
    "# Initialize Vader Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    if text:  \n",
    "        return analyzer.polarity_scores(text)[\"compound\"]\n",
    "    return 0.0  \n",
    "\n",
    "from pyspark.sql.functions import date_format, col\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    if not batch_df.isEmpty():\n",
    "        print(f\"Processing Batch {batch_id}\")\n",
    "\n",
    "        # Ensure 'date_minute' exists before using it\n",
    "        if \"date_minute\" not in batch_df.columns:\n",
    "            print(\"Warning: 'date_minute' column missing in batch. Creating it dynamically...\")\n",
    "            batch_df = batch_df.withColumn(\"date_minute\", date_format(col(\"date\"), \"yyyy-MM-dd HH:mm\"))\n",
    "\n",
    "        # Extract unique 'date_minute' values from batch\n",
    "        unique_dates = batch_df.select(\"date_minute\").distinct()\n",
    "\n",
    "        # Ensure BTC data is also in correct format\n",
    "        btc_filtered_df = btc_stream_df.withColumn(\"date_minute\", col(\"date_minute\").cast(\"string\"))\n",
    "\n",
    "        # Perform join between batch tweets and BTC prices\n",
    "        enriched_df = batch_df.join(btc_filtered_df, on=\"date_minute\", how=\"inner\")\n",
    "\n",
    "        # Apply Sentiment Analysis using Pandas\n",
    "        pdf = enriched_df.toPandas()\n",
    "        pdf[\"sentiment_score\"] = pdf[\"text\"].apply(lambda text: analyzer.polarity_scores(text)[\"compound\"] if text else 0.0)\n",
    "\n",
    "        # Convert back to Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(pdf)\n",
    "\n",
    "        # Select required columns and rename 'Close' as 'btc_price'\n",
    "        final_df = spark_df.select(\"date_minute\", \"favorites\", \"retweets\", \"text\", \"sentiment_score\", col(\"Close\").alias(\"btc_price\"))\n",
    "\n",
    "        # Show results\n",
    "        final_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "670451b2-9eea-4e66-96d0-e0175b6e5aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch 1\n",
      "Warning: 'date_minute' column missing in batch. Creating it dynamically...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 14:30:01 ERROR MicroBatchExecution: Query [id = 36a9c1fb-ec03-498a-888c-880801b13e4e, runId = d4cd1802-c1b6-4294-8c43-3c17628a7ac9] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_29124/1230733154.py\", line 42, in process_batch\n",
      "    pdf = enriched_df.toPandas()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py\", line 202, in toPandas\n",
      "    rows = self.collect()\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/dataframe.py\", line 1257, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Queries with streaming sources must be executed with writeStream.start();\n",
      "delta\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy53.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 36a9c1fb-ec03-498a-888c-880801b13e4e, runId = d4cd1802-c1b6-4294-8c43-3c17628a7ac9] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/utils.py\", line 120, in call\n    raise e\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/utils.py\", line 117, in call\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n  File \"/tmp/ipykernel_29124/1230733154.py\", line 42, in process_batch\n    pdf = enriched_df.toPandas()\n          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py\", line 202, in toPandas\n    rows = self.collect()\n           ^^^^^^^^^^^^^^\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/dataframe.py\", line 1257, in collect\n    sock_info = self._jdf.collectToPython()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.AnalysisException: Queries with streaming sources must be executed with writeStream.start();\ndelta\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m twitter_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(process_batch) \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.jupyter_venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.jupyter_venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = 36a9c1fb-ec03-498a-888c-880801b13e4e, runId = d4cd1802-c1b6-4294-8c43-3c17628a7ac9] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/utils.py\", line 120, in call\n    raise e\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/utils.py\", line 117, in call\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n  File \"/tmp/ipykernel_29124/1230733154.py\", line 42, in process_batch\n    pdf = enriched_df.toPandas()\n          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py\", line 202, in toPandas\n    rows = self.collect()\n           ^^^^^^^^^^^^^^\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/sql/dataframe.py\", line 1257, in collect\n    sock_info = self._jdf.collectToPython()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/home/osbdet/.jupyter_venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.AnalysisException: Queries with streaming sources must be executed with writeStream.start();\ndelta\n"
     ]
    }
   ],
   "source": [
    "query = twitter_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba579d-84c5-467b-9126-41fe7b06933b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
